{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85009210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "import config\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef19f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES_3D = 64\n",
    "TRAINING_BATCH_SIZE = 4\n",
    "TEST_BATCH_SIZE = 4\n",
    "IMAGE_SIZE = 224\n",
    "N_EPOCHS = 15\n",
    "do_valid = True\n",
    "n_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2036730a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    BraTS21ID  MGMT_value\n",
       " 0           0           1\n",
       " 1           2           1\n",
       " 2           3           0\n",
       " 3           5           1\n",
       " 4           6           1\n",
       " 5           8           1\n",
       " 6           9           0\n",
       " 7          11           1\n",
       " 8          12           1\n",
       " 9          14           1\n",
       " 10         17           0\n",
       " 11         18           0\n",
       " 12         19           0\n",
       " 13         20           1\n",
       " 14         21           0\n",
       " 15         22           0\n",
       " 16         24           0\n",
       " 17         25           1\n",
       " 18         26           1\n",
       " 19         28           1,\n",
       " 585)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train_labels.csv\")\n",
    "df[:20] , len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df47a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cropped_image_size(path):\n",
    "    \"\"\"\n",
    "    reading dicom files and returning the resolution after cropping the files using `crop_img` function \n",
    "    resolution : number of pixels in cropped dicom file\n",
    "    \"\"\"\n",
    "    \n",
    "    dicom = pydicom.read_file(path)\n",
    "    data = dicom.pixel_array\n",
    "    cropped_data = crop_img(data)\n",
    "    resolution = cropped_data.shape[0]*cropped_data.shape[1]  \n",
    "    return resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265f146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_img(img):\n",
    "    \n",
    "    \"\"\"\n",
    "    removing zero valued pixels in dicom slice , if the dicom file is all zeros the fucntion returns an empty list\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    c1, c2 = False, False\n",
    "    try:\n",
    "        rmin, rmax = np.where(rows)[0][[0, -1]]        # np.where(rows) : gettin indices of True values (not zero pixels in dicom)\n",
    "    except:                                            # np.where(rows)[0][0,-1] getting the first and the last indices of the non zero pixeles in dicom file  (rmin , rmax)\n",
    "        rmin, rmax = 0, img.shape[0]                   # remove all zeros slices           \n",
    "        c1 = True\n",
    "\n",
    "    try:\n",
    "        cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    except:\n",
    "        cmin, cmax = 0, img.shape[1]\n",
    "        c2 = True\n",
    "    bb = (rmin, rmax, cmin, cmax)\n",
    "    \n",
    "    if c1 and c2:\n",
    "        return img[0:0, 0:0]                           # remove all zeros slices\n",
    "    else:\n",
    "        return img[bb[0] : bb[1], bb[2] : bb[3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf14ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom_image(path, img_size=IMAGE_SIZE, voi_lut=True, rotate=0):\n",
    "    dicom = pydicom.read_file(path)\n",
    "    data = dicom.pixel_array\n",
    "    if voi_lut:\n",
    "        data = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "    else:\n",
    "        data = dicom.pixel_array\n",
    "\n",
    "    if rotate > 0:\n",
    "        rot_choices = [\n",
    "            0,\n",
    "            cv2.ROTATE_90_CLOCKWISE,\n",
    "            cv2.ROTATE_90_COUNTERCLOCKWISE,\n",
    "            cv2.ROTATE_180,\n",
    "        ]\n",
    "        data = cv2.rotate(data, rot_choices[rotate])\n",
    "\n",
    "    data = cv2.resize(data, (img_size, img_size))\n",
    "    data = data - np.min(data)\n",
    "    if np.min(data) < np.max(data):\n",
    "        data = data / np.max(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c55f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainRSNADataset(Dataset):\n",
    "    def __init__(\n",
    "        self, data, transform=None, target=\"MGMT_value\", mri_type=\"T1w\", is_train=True, ds_type=\"forgot\", do_load=True\n",
    "    ):\n",
    "        self.target = target\n",
    "        self.data = data\n",
    "        self.type = mri_type\n",
    "\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.folder = \"train\" if self.is_train else \"test\"\n",
    "        self.do_load = do_load\n",
    "        self.ds_type = ds_type\n",
    "        self.img_indexes = self._prepare_biggest_images()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.loc[index]\n",
    "        case_id = int(row.BraTS21ID)\n",
    "        target = int(row[self.target])\n",
    "        _3d_images = self.load_dicom_images_3d(case_id)\n",
    "        _3d_images = torch.tensor(_3d_images).float()\n",
    "        if self.is_train:\n",
    "            return {\"image\": _3d_images, \"target\": target, \"case_id\": case_id}\n",
    "        else:\n",
    "            return {\"image\": _3d_images, \"case_id\": case_id}\n",
    "\n",
    "    def _prepare_biggest_images(self):\n",
    "        \"\"\"\n",
    "        getting the biggest dicom file from patient scans after cropping zero valued pixels\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        big_image_indexes = {}\n",
    "        if (f\"big_image_indexes_{self.ds_type}_{self.type}.pkl\" in os.listdir(\"indices/\"))\\\n",
    "            and (self.do_load) :\n",
    "            print(\"Loading the best images indexes for all the cases...\")\n",
    "            big_image_indexes = joblib.load(f\"indices/big_image_indexes_{self.ds_type}_{self.type}.pkl\")\n",
    "            return big_image_indexes\n",
    "        else:\n",
    "            \n",
    "            print(\"Caulculating the best scans for every case...\")\n",
    "            for row in tqdm(self.data.iterrows(), total=len(self.data)):\n",
    "                case_id = str(int(row[1].BraTS21ID)).zfill(5)\n",
    "                path = f\"train/{case_id}/{self.type}/*.dcm\"\n",
    "                files = sorted(\n",
    "                    glob.glob(path),\n",
    "                    key=lambda var: [\n",
    "                        int(x) if x.isdigit() else x for x in re.findall(r\"[^0-9]|[0-9]+\", var)\n",
    "                    ],\n",
    "                )\n",
    "                resolutions = [extract_cropped_image_size(f) for f in files]\n",
    "                middle = np.array(resolutions).argmax()\n",
    "                big_image_indexes[case_id] = middle\n",
    "\n",
    "            joblib.dump(big_image_indexes, f\"indices/big_image_indexes_{self.ds_type}_{self.type}.pkl\")\n",
    "            return big_image_indexes\n",
    "\n",
    "\n",
    "\n",
    "    def load_dicom_images_3d(\n",
    "        self,\n",
    "        case_id,\n",
    "        num_imgs=NUM_IMAGES_3D,\n",
    "        img_size=IMAGE_SIZE,\n",
    "        rotate=0,\n",
    "    ):\n",
    "        case_id = str(case_id).zfill(5)\n",
    "\n",
    "        path = f\"{self.folder}/{case_id}/{self.type}/*.dcm\"\n",
    "        files = sorted(\n",
    "            glob.glob(path),\n",
    "            key=lambda var: [\n",
    "                int(x) if x.isdigit() else x for x in re.findall(r\"[^0-9]|[0-9]+\", var)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    \n",
    "        middle = self.img_indexes[case_id]   # largest resolution index of cropped dicom files  (largest cropped dicom)\n",
    "\n",
    "        # # middle = len(files) // 2\n",
    "        num_imgs2 = num_imgs // 2\n",
    "        p1 = max(0, middle - num_imgs2)    # if the largest resultion dicom index less than the half of image depth start from 0\n",
    "        p2 = min(len(files), middle + num_imgs2)  # either you take all files of only half the depth after the largest\n",
    "        image_stack = [load_dicom_image(f, rotate=rotate, voi_lut=True) for f in files[p1:p2]]  #stacking images after one another\n",
    "        \n",
    "        img3d = np.stack(image_stack).T\n",
    "        if img3d.shape[-1] < num_imgs:   # in case all the dicom files are less than the preset `num_imgs` \n",
    "            n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n",
    "            img3d = np.concatenate((img3d, n_zero), axis=-1)\n",
    "\n",
    "        return np.expand_dims(img3d, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cd29f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_{}\n",
      "Loading the best images indexes for all the cases...\n",
      "Loading the best images indexes for all the cases...\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "df = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "train_idx = random.sample(list(range(len(df))), k = 420)\n",
    "\n",
    "train_df = df.iloc[train_idx].reset_index(drop=False)\n",
    "\n",
    "val_idx = [x for x in list(range(len(df))) if x not in train_idx]\n",
    "\n",
    "val_df = df.iloc[val_idx].reset_index(drop=False)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "print(\"train_{}\")\n",
    "train_dataset = BrainRSNADataset(data=train_df, mri_type=\"FLAIR\",ds_type=\"train\")\n",
    "\n",
    "valid_dataset = BrainRSNADataset(data=val_df, mri_type=\"FLAIR\" ,ds_type=\"val\")\n",
    "\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAINING_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=n_workers,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "validation_dl = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.features[0]  = nn.Conv2d(1,64 ,(3,3) , padding=(1,1))\n",
    "net.classifier[6] = nn.Linear(4096 , 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc26708a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 224, 224, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "batch[\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09ebcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net(batch[\"image\"][:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990ec78",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71679681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a013c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        #out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58250ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = RNN(1000 , 1000 , 16 , 2)\n",
    "lstm.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e74a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm(seq.reshape(1,64,1000).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f305a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02adb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.zeros(64,1000)\n",
    "for i in range(64):\n",
    "    seq[i,:] = net(batch[\"image\"][:1,:,:,:,i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56719fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68200e2",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL(nn.Module):\n",
    "      def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # CNN Encoder\n",
    "        cnn = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n",
    "        cnn.features[0]  = nn.Conv2d(1,64 ,(3,3) , padding=(1,1))\n",
    "        cnn.classifier[6] = nn.Linear(4096 , 256)\n",
    "        \n",
    "                \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # CNN Encoding\n",
    "        seq = torch.zeros([TRAINING_BATCH_SIZE,NUM_IMAGES_3D,self.input_size])\n",
    "        for i in range(TRAINING_BATCH_SIZE+1):\n",
    "            for j in range(NUM_IMAGES_3D+1):\n",
    "                seq[i,j,:] = self.cnn(x[\"image\"][:j,:,:,:,i])\n",
    "\n",
    "        \n",
    "        \n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        #out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad93f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "se = torch.zeros_like(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c4391",
   "metadata": {},
   "outputs": [],
   "source": [
    "se.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a75b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49534229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ahmed/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193e4e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dcd7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1 = nn.Conv2d(1,64,(7,7),stride=(2,2),padding=(3,3) , bias=False)\n",
    "model.fc = nn.Linear(512,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d549f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f19f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02b9f635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq = torch.zeros([TRAINING_BATCH_SIZE,NUM_IMAGES_3D,256])\n",
    "for i in range(TRAINING_BATCH_SIZE):\n",
    "    for j in range(NUM_IMAGES_3D):\n",
    "        seq[i,j,:] = model(batch[\"image\"][i,:,:,:,j].unsqueeze(dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c1bd31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 64, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.unsqueeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6326033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432f75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f065f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss(lstm(seq.reshape(1,64,1000).to(device)), torch.tensor([1],device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1721943",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852460a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954d948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_monai(model , lstm , optimizer , train_loader , val_loader , loss_fucntion , device = device , epochs = 20):\n",
    "    val_interval = 2\n",
    "    best_metric = -1\n",
    "    #epoch_loss_values = list()\n",
    "   # metric_values = list()\n",
    "    writer = SummaryWriter()\n",
    "    for epoch in range(5):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{5}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs, labels = batch_data[\"image\"].to(device), batch_data[\"target\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs_cnn = model(inputs)\n",
    "            outputs = lstm(outputs_cnn(TRAINING_BATCH_SIZE,NUM_IMAGES_3D,outputs_cnn.shape[1]))\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_len = len(train_ds) // train_loader.batch_size\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "        epoch_loss /= step\n",
    "        #epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                num_correct = 0.0\n",
    "                metric_count = 0\n",
    "                for val_data in val_loader:\n",
    "                    val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "                    val_outputs = model(val_images)\n",
    "                    value = torch.eq(val_outputs.argmax(dim=1), val_labels)\n",
    "                    metric_count += len(value)\n",
    "                    num_correct += value.sum().item()\n",
    "                metric = num_correct / metric_count\n",
    "                #metric_values.append(metric)\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(model.state_dict(), f\"{best_metric}_best_metric_model_classification3d_array.pth\")\n",
    "                    print(\"saved new best metric model\")\n",
    "                print(\n",
    "                    \"current epoch: {} current accuracy: {:.4f} best accuracy: {:.4f} at epoch {}\".format(\n",
    "                        epoch + 1, metric, best_metric, best_metric_epoch\n",
    "                    )\n",
    "                )\n",
    "                writer.add_scalar(\"val_accuracy\", metric, epoch + 1)\n",
    "    print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x[0] = torch.tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a946cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
